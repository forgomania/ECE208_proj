{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "df_seq=pd.read_csv('pdb_data_seq.csv')\n",
    "df_properties=pd.read_csv('pdb_data_no_dups.csv')\n",
    "df_total=df_seq.merge(df_properties,left_on='structureId',right_on = 'structureId')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['structureId', 'chainId', 'sequence', 'residueCount_x',\n",
       "       'macromoleculeType_x', 'classification', 'experimentalTechnique',\n",
       "       'macromoleculeType_y', 'residueCount_y', 'resolution',\n",
       "       'structureMolecularWeight', 'crystallizationMethod',\n",
       "       'crystallizationTempK', 'densityMatthews', 'densityPercentSol',\n",
       "       'pdbxDetails', 'phValue', 'publicationYear'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select only protein, and filtered by top N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the data in top n by count, top 10?\n",
    "count = df_total['classification'].value_counts(dropna=False)[:10]\n",
    "df_selected=df_total[df_total['classification'].isin(set(count.index))]\n",
    "#we want only protein\n",
    "df_selected=df_selected[df_selected['macromoleculeType_x'].isin(set(['Protein']))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select proteins with only one chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select proteins with only one chain in the data set\n",
    "#how to justify this operation?\n",
    "#df_onechain = df_selected[df_selected.groupby('structureId').structureId.transform(len) == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df_selected[['structureId','classification','sequence']]\n",
    "#test_df = df_onechain[['structureId','classification','sequence']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to be done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further select data and simplify problem, select proteins with only one chain?\n",
    "\n",
    "\n",
    "Figure out how to convert sequence data into array and training model afterwards.\n",
    "\n",
    "\n",
    "More models and discussion (*LSTM)\n",
    "\n",
    "Models build on features other than sequence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier,ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional, take part of the data for faster verification\n",
    "data = test_df.sample(50000)\n",
    "\n",
    "#need to remove nulls\n",
    "data = data.dropna()\n",
    "X_train, X_test,y_train,y_test = \\\n",
    "train_test_split(data['sequence'], data['classification'], test_size = 0.1, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction From Sequence Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorize data, prepare for building models\n",
    "#Convert a collection of text documents to a matrix of token counts\n",
    "#seems has nothing to do with sequence but only with the frequency\n",
    "\n",
    "#ngram is a parameter we need to focus on, \n",
    "\n",
    "#vect = CountVectorizer(analyzer = 'char_wb', ngram_range = (3,3))\n",
    "vect = CountVectorizer(analyzer = 'char_wb')\n",
    "#vect =TfidfVectorizer(analyzer = \"char_wb\",sublinear_tf= True )\n",
    "# Fit and Transform CountVectorizer\n",
    "#occasionally may meet np.nan error\n",
    "vect.fit(X_train)\n",
    "X_train_df = vect.transform(X_train)\n",
    "X_test_df = vect.transform(X_test)\n",
    "\n",
    "#to store the results for different mothods\n",
    "prediction = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3082\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(X_train_df, y_train)\n",
    "#test on test set\n",
    "NB_pred = model.predict(X_test_df)\n",
    "prediction[\"MultinomialNB\"] = accuracy_score(NB_pred, y_test)\n",
    "print( prediction['MultinomialNB'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3604\n"
     ]
    }
   ],
   "source": [
    "model = AdaBoostClassifier()\n",
    "model.fit(X_train_df,y_train)\n",
    "ADA_pred = model.predict(X_test_df)\n",
    "prediction[\"Adaboost\"] = accuracy_score(ADA_pred , y_test)\n",
    "print(prediction[\"Adaboost\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\ericl\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8694\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier()\n",
    "model.fit(X_train_df,y_train)\n",
    "ADA_pred = model.predict(X_test_df)\n",
    "prediction[\"Random_Forest\"] = accuracy_score(ADA_pred , y_test)\n",
    "print(prediction[\"Random_Forest\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<45000x23 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 906670 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5000x23 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 100749 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(9736, 2000)\n",
    "        self.bc1 = nn.BatchNorm1d(2000)\n",
    "        \n",
    "        self.fc2 = nn.Linear(2000, 252)\n",
    "        self.bc2 = nn.BatchNorm1d(252)\n",
    "        \n",
    "        self.fc3 = nn.Linear(252, 10)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view((-1, 9736))\n",
    "        h = self.fc1(x)\n",
    "        h = self.bc1(h)\n",
    "        h = F.relu(h)\n",
    "        #h = F.dropout(h, p=0.5, training=self.training)\n",
    "        \n",
    "        h = self.fc2(h)\n",
    "        h = self.bc2(h)\n",
    "        h = F.relu(h)\n",
    "        #h = F.dropout(h, p=0.2, training=self.training)\n",
    "        \n",
    "        h = self.fc3(h)\n",
    "        out = F.log_softmax(h,dim = 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "model.cuda() # CUDA!\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data for pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert y labels to index encoding\n",
    "y_train_array = np.zeros(162111)\n",
    "for i in range(len(y_train)):\n",
    "    cur_key = y_train.iloc[i]\n",
    "    idx = cls.index(cur_key)\n",
    "    y_train_array[i]=idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_array = np.zeros(18013)\n",
    "for i in range(len(y_test)):\n",
    "    cur_key = y_test.iloc[i]\n",
    "    idx = cls.index(cur_key)\n",
    "    y_test_array[i]=idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "losses = []\n",
    "for epoch in range(5):\n",
    "    \n",
    "    for i in range(1600):\n",
    "        data = torch.from_numpy(X_train_df[i:i+100].toarray()).float().cuda()\n",
    "        target = torch.from_numpy(y_train_array[i:i+100]).to(device = device, dtype=torch.int64)\n",
    "        #print(data)\n",
    "        # Get Samples\n",
    "        data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        #target = target.squeeze(1)\n",
    "        # Init\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Predict\n",
    "        #print(type(data))\n",
    "        y_pred = model(data) \n",
    "        #print(y_pred)\n",
    "        # Calculate loss\n",
    "        loss = criterion(y_pred, target)\n",
    "        losses.append(loss.item())\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        # Display\n",
    "        if i % 100 == 1:\n",
    "            print(loss.item())\n",
    "            \n",
    "    print()\n",
    "    \n",
    "    \n",
    "    #test on test set\n",
    "    test_acc = 0\n",
    "    for i in range(180):\n",
    "        data = torch.from_numpy(X_test_df[i:i+100].toarray()).float().cuda()\n",
    "        target = torch.from_numpy(y_test_array[i:i+100]).to(device = device, dtype=torch.int64)\n",
    "        \n",
    "        y_pred = model(data)\n",
    "        _,predicted = torch.max(y_pred,1)\n",
    "        test_acc += (predicted == target).sum().item()\n",
    "        \n",
    "        #print('temp accuracy ',(predicted == target).sum().item()/100)\n",
    "        \n",
    "    print('current test_accuracy',test_acc/18000)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another vectoriation method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import unicodedata\n",
    "import string\n",
    "import torch\n",
    "all_letters = string.ascii_letters[26:]\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "def findFiles(path): return glob.glob(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ABCDEFGHIJKLMNOPQRSTUVWXYZ'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_lines = {}\n",
    "all_categories = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_categories= list(count.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning seq to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X_train)):\n",
    "    if y_train.iloc[i] not in category_lines:\n",
    "        category_lines[y_train.iloc[i]] = [X_train.iloc[i]]\n",
    "    else:\n",
    "        category_lines[y_train.iloc[i]].append(X_train.iloc[i])\n",
    "n_categories = len(category_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(letterToTensor('J'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        #self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers=num_layers)\n",
    "        \n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "    \n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM,self).__init__()\n",
    "        \n",
    "        \n",
    "        self.input_dim = input_size\n",
    "        self.hidden_dim = hidden_size\n",
    "        self.num_layers = 1\n",
    "        \n",
    "        self.input2hidden = nn.Linear(input_size,hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
    "        self.hidden2tag = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        h = Variable(torch.zeros(self.num_layers, 1, self.hidden_dim))\n",
    "        c = Variable(torch.zeros(self.num_layers, 1, self.hidden_dim))\n",
    "        return h,c\n",
    "\n",
    "\n",
    "    def forward(self, seq):\n",
    "        output = self.input2hidden(seq)\n",
    "        #print(self.hidden)\n",
    "        #print(output.view(len(seq),1,-1))\n",
    "        output,self.hidden = self.lstm(output.view(len(seq),1,-1),self.hidden)\n",
    "        #print(output)\n",
    "        #print(self.hidden)\n",
    "        output = self.hidden2tag(output)[0]\n",
    "        #print(output)\n",
    "        #print(output)\n",
    "        \n",
    "        return self.softmax(output)\n",
    "n_hidden = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(n_letters, n_hidden, n_categories)#.to('cuda:0')\n",
    "#rnn = LSTM(n_letters, n_hidden, n_categories).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return all_categories[category_i], category_i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category = VIRUS / sequence = TDAPVSKASTVTGFGRGTNDVHLSGMSRISQAVLPAGTGTDGYVVVDATIVPDLLPRLGHAARIFQRYAVETLEFEIQPMCPANTGGGYVAGFLPDPTDNDHTFDALQATRGAVVAKWWESRTVRPQYTRTLLWTSSGKEQRLTSPGRLILLCVGNNTDVVNVSVLCRWSVRLSVPSLENPEE\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "def randomTrainingExample():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)#.to('cuda:0')\n",
    "    line_tensor = lineToTensor(line)#.to('cuda:0')\n",
    "    return category, line, category_tensor, line_tensor\n",
    "\n",
    "category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "print('category =', category, '/ sequence =', line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▉                                                                            | 498/20000 [00:40<22:01, 14.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 2% (0m 40s) 2.2825 MENGKRDRQDMEVNTTPRKPRVLLAASGSVAAIKFGNLCHCFTEWAEVRAVVTKSSLHFLDKLSLPQEVTLYTDEDEWSSWNKIGDPVLHIELRRWADVLVIAPLSANTLGKIAGGLCDNLLTCIIRAWDYTKPLFVAPAMNTLMWNNPFTERHLLSLDELGITLIPPIKKRLASGDYGNGAMAEPSLIYSTVRLFWESQAHQQTGGTS / TRANSCRIPTION ✗ (LYASE)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|███▉                                                                          | 997/20000 [01:18<20:22, 15.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 5% (1m 18s) 2.4773 EVQLVESGGGLVKPGGSLRLSCVGSEFTFSDAWMTWVRQAPGKGLEWVGHMRPTPEGGAKDYAAPVKGRFTVSRDDSKRTLYLQMNSLKIEDTAVYYCMTGVEKGDFWSDDYSQHYNTYLIDVWGKGTTVTVSSASTKGPSVFPLAPSSKSTSGGTAALGCLVKDYFPEPVTVSWNSGALTSGVHTFPAVLQSSGLYSLSSVVTVPSSSLGTQTYICNVNHKPSNTKVDKRVEPKSCDHHHHHH / OXIDOREDUCTASE ✗ (IMMUNE SYSTEM)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████▊                                                                       | 1499/20000 [02:00<26:42, 11.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500 7% (2m 0s) 2.3854 GHMPDLADLFPGFGSEWINTSSGRIFARVGGDGPPLLLLHGFPQTHVMWHRVAPKLAERFKVIVADLPGYGWSDMPESDEQHTPYTKRAMAKQLIEAMEQLGHVHFALAGHDRGARVSYRLALDSPGRLSKLAVLDILPTYEYWQRMNRAYALKIYHWSFLAQPAPLPENLLGGDPDFYVKAKLASWTRAGDLSAFDPRAVEHYRIAFADPMRRHVMCEDYRAGAYADFEHDKIDVEAGNKIPVPMLALWGASGIAQSAATPLDVWRKWASDVQGAPIESGHFLPEEAPDQTAEALVRFFSAAPGS / LYASE ✗ (HYDROLASE)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████▋                                                                     | 1998/20000 [02:39<22:35, 13.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 10% (2m 39s) 2.2282 KREAEARWRQTWSGPGTTKRFPETVLARCVKYTEIHPEMRHVDCQSVWDAFKGAFISKHPCDITEEDYQPLMKLGTQTVPCNKILLWSRIKDLAHQFTQVQRDMFTLEDTLLGYLADDLTWCGEFDTSKINYQSCPDWRKDCSNNPVSVFWKTVSRRFAEAACDVVHVMLDGSRSKIFDKDSTFGSVQVHNLQPEKVQTLEAWVIHGGREDSRDLCQDPTIKELESIISKRNIQFSCKNIYRPDKFLQCVKNPEDSSCTSEI / LYASE ✗ (HYDROLASE)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████▌                                                                   | 2498/20000 [03:18<24:00, 12.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500 12% (3m 18s) 2.4538 DIQMTQTTSSLSASLGDRVTISCRASQDITNYLNWYQQKPDGTVKLLIYYTSRLHSGVPSRFSGSGSGTDYSLTISNLEQEDIATYFCQQGKTLPTFGGGTKLEIKRADAAPTVSIFPPSSEQLTSGGASVVCFLNNFYPKDINVKWKIDGSERQNGVLNSWTDQDSKDSTYSMSSTLTLTKDEYERHNSYTCEATHKTSTSPIVKSFNR / VIRAL PROTEIN ✗ (IMMUNE SYSTEM)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|███████████▌                                                                 | 2999/20000 [03:57<25:02, 11.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 15% (3m 57s) 2.2679 MGPLQYKDLKIDIKTSPPPECINDLLQAVDSQEVRDYCEKKGWIVNITSQVQTERNINRA / LYASE ✗ (VIRAL PROTEIN)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█████████████▍                                                               | 3499/20000 [04:34<21:47, 12.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3500 17% (4m 34s) 2.1735 MKLKTTLFGNVYQFKDVKEVLAKANELRSGDVLAGVAAASSQERVAAKQVLSEMTVADIRNNPVIAYEDDCVTRLIQDDVNETAYNQIKNWSISELREYVLSDETSVDDIAFTRKGLTSEVVAAVAKICSNADLIYGAKKMPVIKKANTTIGIPGTFSARLQPNDTRDDVQSIAAQIYEGLSFGVGDAVIGVNPVTDDVENLSRVLDTIYGVIDKFNIPTQGCVLAHVTTQIEAIRRGAPGGLIFQSICGSEKGLKEFGVELAMLDEARAVGAEFNRIAGENCLYFETGQGSALSAGANFGADQVTMEARNYGLARHYDPFIVNTVVGFIGPEYLYNDRQIIRAGLEDHFMGKLSGISMGCDCCYTNHADADQNLNENLMILLATAGCNYIMGMPLGDDIMLNYQTTAFHDTATVRQLLNLRPSPEFERWLESMGIMANGRLTKRAGDPSLFF / RIBOSOME ✗ (LYASE)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████▍                                                             | 3998/20000 [05:07<14:59, 17.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000 20% (5m 7s) nan EVTLQESGGGLVQPGGSMKLSCAASGFTFSDAWVDWVRQSPGKGLEWVAEIRNKANNHATKYTESVKGRFTISRDDSKSSVYLQMNSLRAEDTGIYYCTSVPQLGRGFAYWGQGTLVTVSAASTTPPSVYPLAPGSGGASTSGSMVTLGCLVKGYFPEPVTVTWNSGALSSGVHTFPAVLNGDLYTLSSSVTVPSSTWPSQTVTCNVAHPASSTQVDKKIVPK / VIRAL PROTEIN ✗ (IMMUNE SYSTEM)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|█████████████████▎                                                           | 4499/20000 [05:38<13:26, 19.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4500 22% (5m 38s) nan PQVTLWQRPLVTIKIGGQLKEALLDTGADDTVLEEMSLPGRWKPKMIGGIGGFIKVRQYDQILIEICGHKAIGTVLVGPTPVNIIGRNLLTQIGATLNF / VIRAL PROTEIN ✗ (HYDROLASE/HYDROLASE INHIBITOR)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|███████████████████▏                                                         | 4999/20000 [06:08<14:16, 17.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 25% (6m 8s) nan VPL / VIRAL PROTEIN ✗ (HYDROLASE)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|█████████████████████▏                                                       | 5496/20000 [06:36<13:03, 18.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5500 27% (6m 36s) nan ASAAQSTPITGKVTAVIGAIVDVHFEQSELPAILNALEIKTPQGKLVLEVAQHLGENTVRTIAMDGTEGLVRGEKVLDTGGPISVPVGRETLGRIINVIGEPIDERGPIKSKLRKPIHADPPSFAEQSTSAEILETGIKVVDLLAPYARGGKIGLFGGAGVGKTVFIQELINNIAKAHGGFSVFTGVGERTREGNDLYREMKETGVINLEGESKVALVFGQMNEPPGARARVALTGLTIAEYFRDEEGQDVLLFIDNIFRFTQAGSEVSALLGRIPSAVGYQPTLATDMGLLQERITTTKKGSVTSVQAVYVPADDLTDPAPATTFAHLDATTVLSRGISELGIYPAVDPLDSKSRLLDAAVVGQEHYDVASKVQETLQTYKSLQDIIAILGMDELSEQDKLTVERARKIQRFLSQPFAVAEVFTGIPGKLVRLKDTVASFKAVLEGKYDNIPEHAFYMVGGIEDVVAKAEKLAAEAN / VIRAL PROTEIN ✗ (HYDROLASE)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████████████████                                                      | 5998/20000 [07:06<13:20, 17.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000 30% (7m 6s) nan MSKIEKLSILGVRSFGPHHPETIAFNTPLTLIVGYNGSGKTTVIECLKYATTGELPPNSTRNGAFIHDPDLVGEKEVRAQVKLSFRSTIGESYVVTRNIQLLVQRNNKRTQKTLEGSLLLRNNGERTVISTRVAELDKLVSEKLGVPPAILDAVIFCHQDDSLWPMSEPAALKKRFDEIFEAQKYTKVIENIRLLKKKKGDELKILKEREVQDKANKERAEKVDGGAGGAGGELDLKDAKAKYKETHIKVETTKAAIEDLGRGMAAVDHAIMQYHSKMMEQINRTIAELWQSTYQGTDIDTIQIRSDVESTTSSDSGTRRNYNYRVSMVKGDTEMDMRGRCSAGQKVLASIIIRLALAESFCANCGLIALDEPTTNLDSDNIRSLAESLHGIIKARQAQGNLQLIVITHDEEFLKYMQCSDFCDDFYRVKRDEKQNSVIVRESITRITE / VIRAL PROTEIN ✗ (HYDROLASE)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|████████████████████████▎                                                    | 6300/20000 [07:26<16:23, 13.93it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-dc78196d509c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0mcategory_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcategory_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0mline_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m     \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategory_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m     \u001b[0mcurrent_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-dc78196d509c>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(category_tensor, line_tensor)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\ericl\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \"\"\"\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\ericl\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "import math\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "n_iters = 20000\n",
    "print_every = 500\n",
    "plot_every = 1000\n",
    "\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "learning_rate = 0.01 # If you set this too high, it might explode. If too low, it might not learn\n",
    "import progressbar\n",
    "progress = progressbar.ProgressBar()\n",
    "def train(category_tensor, line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "\n",
    "    return output, loss.item()\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in tqdm(range(1, n_iters + 1)):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    category_tensor = category_tensor\n",
    "    line_tensor = line_tensor\n",
    "    output, loss = train(category_tensor, line_tensor)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print iter number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess, guess_i = categoryFromOutput(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RIBOSOME'"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guess_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'category_i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-399-93d59be3df52>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcategory_i\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'category_i' is not defined"
     ]
    }
   ],
   "source": [
    "category_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "import progressbar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 32\n",
    "        self.hidden_size = 12\n",
    "        self.embedding_size = 24\n",
    "        self.epochs = 50\n",
    "        self.nr_classes =2\n",
    "        self.gpu = False\n",
    "        self.learning_rate = 0.001\n",
    "        self.train_ratio = 0.82\n",
    "        self.val_ratio = 0.1\n",
    "        self.test_ratio = 1-self.val_ratio-self.train_ratio\n",
    "        self.n_layers = 2\n",
    "        self.dropout = 0.3\n",
    "\n",
    "\n",
    "Args = Params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, n_layers, dropout, embed_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.embed_size = embed_size\n",
    "        self.embedding = nn.Embedding(self.input_size, self.embed_size)\n",
    "        self.rnn = nn.LSTM(input_size=self.embed_size,\n",
    "                           hidden_size=hidden_size,\n",
    "                           dropout=dropout,\n",
    "                           num_layers=n_layers, bidirectional=True)\n",
    "        self.hidden2label = nn.Linear(2*hidden_size, 10)\n",
    "        #self.hidden = self.init_hidden()\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.dropoutLayer = nn.Dropout()\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h0 = Variable(torch.zeros(self.n_layers*2, batch_size, self.hidden_size))\n",
    "        c0 = Variable(torch.zeros(self.n_layers*2, batch_size, self.hidden_size))\n",
    "        return h0, c0\n",
    "\n",
    "    def forward(self, inputs, input_lengths):  # inputs: antal i batch x max ord längd\n",
    "        self.hidden = self.init_hidden(inputs.size(-1)) # -1 if batch_first=False\n",
    "        embedded = self.embedding(inputs)  # antal i batch * max ord längd * Embedding_dim\n",
    "        packed = pack_padded_sequence(embedded, input_lengths, batch_first=False)  # packad\n",
    "        outputs, self.hidden = self.rnn(packed, self.hidden)  #(packed, self.hidden)  #\n",
    "        output, output_lengths = pad_packed_sequence(outputs, batch_first=False)\n",
    "        #print(output.size())\n",
    "        output = torch.transpose(output, 0, 1)\n",
    "        #print(output.size())\n",
    "        output = torch.transpose(output, 1, 2)\n",
    "        #print(output.size())\n",
    "        output = torch.tanh(output)\n",
    "        #print(output.size())\n",
    "        output, indices = F.max_pool1d(output,output.size(2), return_indices=True)\n",
    "        #print(output.size())\n",
    "        output = torch.tanh(output)\n",
    "        output = output.squeeze(2)\n",
    "        output = self.dropoutLayer(output)\n",
    "        #print(output.size())\n",
    "        output = self.hidden2label(output)\n",
    "        #print(output.size())\n",
    "        output = self.softmax(output)\n",
    "        return output, self.hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, input_sizes, target_tensor):\n",
    "    model.hidden = model.init_hidden(Args.batch_size) \n",
    "    model.zero_grad()\n",
    "    output, hidden = model.forward(input_tensor, input_sizes)\n",
    "    loss = criterion(output, target_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM_classifier(vocab_size = n_letters, \n",
    "                        hidden_size = Args.hidden_size, \n",
    "                        n_layers = Args.n_layers, \n",
    "                        dropout = Args.dropout,\n",
    "                        embed_size = Args.embedding_size).cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = y_train.iloc[list(range(0,10))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['HYDROLASE', 'TRANSFERASE', 'HYDROLASE', 'OXIDOREDUCTASE',\n",
       "       'HYDROLASE', 'HYDROLASE', 'LYASE', 'LYASE', 'HYDROLASE',\n",
       "       'TRANSFERASE'], dtype=object)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map['TRANSFERASE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 1, 3, 1, 1, 5, 5, 1, 2, 1, 1, 1, 1, 4, 1, 2, 2, 5, 2, 1, 1, 1, 2,\n",
      "        2, 4, 4, 9, 5, 1, 1, 1], dtype=torch.int32)\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have scalar type Long; but got CPUType instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-218-4e81fffb372c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m         output, loss = train(input_tensor=cur_x, \n\u001b[0;32m     35\u001b[0m                      \u001b[0minput_sizes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_lengths\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m                      target_tensor=cur_y)\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[0mcurrent_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-189-e0e7058ebac2>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(input_tensor, input_sizes, target_tensor)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mArgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-188-76199b4ebc31>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs, input_lengths)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# inputs: antal i batch x max ord längd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# -1 if batch_first=False\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0membedded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# antal i batch * max ord längd * Embedding_dim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mpacked\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpack_padded_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# packad\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpacked\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#(packed, self.hidden)  #\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\ericl\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\ericl\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m         return F.embedding(\n\u001b[0;32m    116\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\ericl\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   1504\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1505\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1506\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have scalar type Long; but got CPUType instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.autograd import Variable\n",
    "\n",
    "current_loss = 0\n",
    "\n",
    "all_losses = []\n",
    "val_acc = []\n",
    "train_acc = []\n",
    "predictions = []\n",
    "labels = []\n",
    "#print_every = int(len(train_data)/Args.batch_size) #every epoch\n",
    "#n_iters  = int(len(train_data)/Args.batch_size*Args.epochs)\n",
    "#plot_every = print_every\n",
    "start = time.time()\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "label_map = dict(zip(list(count.index),list(range(10))))\n",
    "for i in range(epochs):\n",
    "    for idx in range(len(X_train)%batch_size):\n",
    "        cur_x = X_train.iloc[list(range(idx,idx+batch_size))].values\n",
    "        temp_y = y_train.iloc[list(range(idx,idx+batch_size))].values\n",
    "        #print(type(cur_y))\n",
    "        #print(cur_y[0])\n",
    "        cur_x = lineToTensor(cur_x)\n",
    "        cur_y = np.zeros(batch_size)\n",
    "        for i in range(len(cur_y)):\n",
    "            cur_y[i] = label_map[temp_y[i]]\n",
    "        #print(type(cur_y))\n",
    "        cur_y = torch.from_numpy(cur_y.astype(np.int))\n",
    "        input_lengths = [len(x) for x in cur_x]\n",
    "        print(cur_y)\n",
    "        #print(cur_x)\n",
    "        print(input_lengths)\n",
    "        output, loss = train(input_tensor=cur_x, \n",
    "                     input_sizes=input_lengths, \n",
    "                     target_tensor=cur_y)\n",
    "        current_loss += loss\n",
    "        predictions.extend(list(torch.argmax(output, dim=1).numpy()))\n",
    "        #labels.extend(list(target_tensor.numpy()))\n",
    "\n",
    "                \n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
